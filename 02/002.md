1. On the master node, determine the health of the cluster by probing the API endpoint

```bash
curl -k https://localhost:6443/healthz?verbose
```

1. ·

```bash
sudo apt-mark unhold kubeadm
sudo apt-get install --only-upgrade kubeadm
sudo kubeadm upgrade plan
kubeadm upgrade apply v1.20.2
sudo apt-get install --only-upgrade kubelet
kubectl create configmap mycm --from-literal=owner=david
kubectl create secret generic mysecret --from-literal=dbuser="MyDatabaseUser" --from-literal=dbpassword="MyDatabasePassword"
```

```
spec:
  replicas: 6
  strategy:
    rollingUpdate:
      maxSurge: 25%
      maxUnavailable: 25%
```

```yml
      env:
        # Define the environment variable
        - name: OWNER
          valueFrom:
            # ref在编程中通常表示引用（Reference）
            configMapKeyRef:
              # The ConfigMap containing the value you want to assign to SPECIAL_LEVEL_KEY
              name: mycm
              # Specify the key associated with the value
              key: owner
```

```yml
    env:
      - name: dbuser
        valueFrom:
          secretKeyRef:
            name: mysecret
            key: dbuser
```

# Excerise 5 - Understand how resource limits can affect Pod scheduling

1. Create a new namespace called "tenant-b-100mi"
2. Create a memory limit of 100Mi for this namespace
3. Create a pod with a memory request of 150Mi, ensure the limit has been set by verifying you get a error message.

```bash
kubectl create ns tenant-b-100mi
```

```bash
apiVersion: v1
kind: LimitRange
metadata:
  name: tenant-b-memlimit
  namespace: tenant-b-100mi
spec:
  limits:
  - max:
      memory: 100Mi
    type: Container
```

```bash
apiVersion: v1
kind: Pod
metadata:
  name: default-mem-demo
  namespace: tenant-b-100mi
spec:
  containers:
  - name: default-mem-demo
    image: nginx
    resources:
      requests:
        memory: 150Mi
```

Which should return:

````bash
The Pod "default-mem-demo" is invalid: spec.containers[0].resources.requests: Invalid value: "150Mi": must be less than or equal to memory limit
````

```bash
[Service Name].[Namespace].[Type].[Base Domain Name]
nginx-service.default.svc.cluster.local
```

# Exercise 3 - Know how to use Ingress controllers and Ingress resources

1. Create an `ingress` object named `myingress` with the following specification:

- Manages the host `myingress.mydomain`
- Traffic to the base path `/` will be forwarded to a `service` called `main` on port 80
- Traffic to the path `/api` will be forwarded to a `service` called `api` on port 8080

Answer - Imperative

```
kubectl create ingress myingress --rule="myingress.mydomain/=main:80" --rule="myingress.mydomain/api=api:8080"
```

Answer - Declarative

Apply the following YAML:

```yml
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  creationTimestamp: null
  name: myingress
spec:
  rules:
  - host: myingress.mydomain
    http:
      paths:
      - backend:
          service:
            name: main
            port:
              number: 80
        path: /
        pathType: Exact
      - backend:
          service:
            name: api
            port:
              number: 8080
        path: /api
        pathType: Exact
status:
  loadBalancer: {}
```

# Exercise 2 - Know how to configure applications with persistent storage

1. Create a

   ```
   persistentVolume
   ```

   object of type

   ```
   hostPath
   ```

   with the following parameters:

   1. 1GB Capacity
   2. Path on the host is /tmp
   3. `storageClassName` is Manual
   4. `accessModes` is `ReadWriteOnce`

2. Create a `persistentVolumeClaim` to the aforementioned `persistentVolume`

3. Create a `pod` workload to leverage this `persistentVolumeClaim

```yml
apiVersion: v1
kind: PersistentVolume
metadata:
 name: pv-hostpath-1gb
spec:
 capacity:
   storage: 1Gi
 volumeMode: Filesystem
 accessModes:
   - ReadWriteOnce
 persistentVolumeReclaimPolicy: Recycle
 storageClassName: manual
 hostPath:
   path: /tmp
---
kind: PersistentVolumeClaim
apiVersion: v1
metadata:
 name: pvc-hostpath-claim
spec:
 accessModes:
   - ReadWriteOnce
 volumeMode: Filesystem
 resources:
   requests:
     storage: 512Mi
 storageClassName: manual
---
apiVersion: v1
kind: Pod
metadata:
 name: pod-with-pvc
spec:
 volumes:
   - name: myvol
     persistentVolumeClaim:
      claimName: pvc-hostpath-claim
 containers:
 - name: busybox
   image: busybox
   args:
   - sleep
   - "1000000"
   volumeMounts:
     - mountPath: "/mnt/readonly"
       name: myvol
```

# Exercise 4 - Troubleshoot networking

```yml
cat << eof | k apply -f - 
---
kind: Service
apiVersion: v1
metadata:
  name: nginx-service2
spec:
  selector:
    app: nginx
  ports:
  - name: "port1"
    protocol: TCP
    port: 80
    targetPort: 8080
  type: ClusterIP
...
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx-deployment
  labels:
    app: nginx
spec:
  replicas: 3
  selector:
    matchLabels:
      app: nginx
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
      - name: nginx
        image: nginx:1.7.9
        ports:
        - containerPort: 80
...
eof
```

```yml
apiVersion: v1
kind: Pod
metadata:
  name: dnsutils
  namespace: default
spec:
  containers:
  - name: dnsutils
    image: yellowo/dnsutils:v1
    command:
      - sleep
      - "infinity"
    imagePullPolicy: IfNotPresent
  restartPolicy: Always
```

```bash
kubectl exec -it dnsutils sh
nslookup nginx-service2.default.svc.cluster.local
kubectl run curl --image=radial/busyboxplus:curl -i --tty
curl nginx-service2.default.svc.cluster.local
curl nginx-service.default.svc.cluster.local
```

```bash
k completion --help
yum -y install bash-completion
```



###### Which suffix will static pods have that run on cluster1-node1?

The suffix is the node hostname with a leading hyphen. It used to be `-static` in earlier Kubernetes versions.

```
k -n project-c13 get pod \
  -o jsonpath="{range .items[*]} {.metadata.name}{.spec.containers[*].resources}{'\n'}"
```

```
k get pods -n project-c13 \
  -o jsonpath="{range .items[*]}{.metadata.name} {.status.qosClass}{'\n'}"
```

```
iptables-save | grep p2-service
```

```
➜ ssh cluster2-controlplane1
➜ root@cluster2-controlplane1:~# vim /etc/kubernetes/manifests/kube-apiserver.yaml
```

```
➜ root@cluster2-controlplane1:~# vim /etc/kubernetes/manifests/kube-controller-manager.yaml
```

