## 署应用程序的方式上，经历了如下的三个阶段

![Kubernetes教程_部署方式演化](https://kuboard.cn/assets/img/container_evolution.cb55fb19.svg)

## k8s 的应用组件

**kube-apiserver**：中间件一样

**etcd**：存储集群配置信息

**kube-scheduler**：管理调度的

**kube-controller-manager**: 管理调谐的

## 基础负载 deployment

> 控制器 Deployment/StatefulSet/DaemonSet 中，restartPolicy只支持 Always 这一个选项，不支持 OnFailure 和 Never 选项。

```yml
apiVersion: apps/v1	#与k8s集群版本有关，使用 kubectl api-versions 即可查看当前集群支持的版本
kind: Deployment	#该配置的类型，我们使用的是 Deployment
metadata:	        #译名为元数据，即 Deployment 的一些基本属性和信息
  name: nginx-deployment	#Deployment 的名称
  labels:	    #标签，可以灵活定位一个或多个资源，其中key和value均可自定义，可以定义多组，目前不需要理解
    app: nginx	#为该Deployment设置key为app，value为nginx的标签
spec:	        #这是关于该Deployment的描述，可以理解为你期待该Deployment在k8s中如何使用
  replicas: 1	#使用该Deployment创建一个应用程序实例
  selector:	    #标签选择器，与上面的标签共同作用，目前不需要理解
    matchLabels: #选择包含标签app:nginx的资源
      app: nginx
  template:	    #这是选择或创建的Pod的模板
    metadata:	#Pod的元数据
      labels:	#Pod的标签，上面的selector即选择包含标签app:nginx的Pod
        app: nginx
    spec:	    #期望Pod实现的功能（即在pod中部署）
      containers:	#生成container，与docker中的container是同一种
      - name: nginx	#container的名称
        image: nginx:1.7.9	#使用镜像nginx:1.7.9创建container，该container默认80端口可访问
```

```yml
apiVersion: v1
kind: Service
metadata:
  name: nginx-service	#Service 的名称
  labels:     	#Service 自己的标签
    app: nginx	#为该 Service 设置 key 为 app，value 为 nginx 的标签
spec:	    #这是关于该 Service 的定义，描述了 Service 如何选择 Pod，如何被访问
  selector:	    #标签选择器
    app: nginx	#选择包含标签 app:nginx 的 Pod
  ports:
  - name: nginx-port	#端口的名字
    protocol: TCP	    #协议类型 TCP/UDP
    port: 80	        #集群内的其他容器组可通过 80 端口访问 Service
    nodePort: 32600   #通过任意节点的 32600 端口访问 Service
    targetPort: 80	#将请求转发到匹配 Pod 的 80 端口
  type: NodePort	#Serive的类型，ClusterIP/NodePort/LoaderBalancer
```

##  并非所有对象都在名称空间里

```bash
# 在名称空间里
kubectl api-resources --namespaced=true
# 不在名称空间里
kubectl api-resources --namespaced=false
```

## 切换上下文

```bash
kubectl config current-context
kubectl config use-context dev
```

## 标签选择器

```bash
kubectl get pods -l environment=production,tier=frontend
kubectl get pods -l 'environment in (production),tier in (frontend)'
```

```yml
selector:
  matchLabels:
    component: redis
  matchExpressions:
    - {key: tier, operator: In, values: [cache]}
    - {key: environment, operator: NotIn, values: [dev]}
```

## 字段选择器

字段选择器（Field Selector）就是使用资源对应的yaml中对应的字段来进行筛选

```bash
kubectl get pods --field-selector status.phase=Running
kubectl get services  --all-namespaces --field-selector metadata.namespace!=default
kubectl get pods --field-selector=status.phase!=Running,spec.restartPolicy=Always
kubectl get statefulsets,services --all-namespaces --field-selector metadata.namespace!=default
```

## pod 运行状态

| Phase                      | 描述                                                         |
| -------------------------- | ------------------------------------------------------------ |
| Pending                    | Pod 还未完成调度（例如没有合适的节点）,还未开始执行初始化容器 |
| Init:N/M                   | Pod 中包含 M 个初始化容器，其中 N 个初始化容器已经成功执行   |
| Init:Error                 | Pod 中有一个初始化容器执行失败                               |
| Init:CrashLoopBackOff      | Pod 中有一个初始化容器反复执行失败                           |
| PodInitializing or Running | Pod 已经完成初始化容器的执行                                 |
| Running                    | Pod 已经被绑定到一个节点，并且该 Pod 所有的容器都已经成功创建。其中至少有一个容器正在运行，或者正在启动/重启 |
| Succeeded                  | Pod 中的所有容器都已经成功终止，并且不会再被重启             |
| Failed                     | Pod 中的所有容器都已经终止，至少一个容器终止于失败状态：容器的进程退出码不是 0，或者被系统 kill |
| Unknown                    | api-server 与 Pod 所在节点之间的通信故障,大概是kubelet或这是api-server 还没完全启动 |

## ReplicaSet

```yml
apiVersion: apps/v1
kind: ReplicaSet
metadata:
  name: frontend
  labels:
    app: guestbook
    tier: frontend
spec:
  # modify replicas according to your case
  replicas: 3
  selector:
    matchLabels:
      tier: frontend
  template:
    metadata:
      labels:
        tier: frontend
    spec:
      containers:
      - name: nginx
        image: nginx
```

## 获取Non-Template Pod

由于您可以直接创建 Pod，Kubernetes 中强烈建议在您直接创建的 Pod 中，其标签不会与任何一个 ReplicaSet 的标签匹配。原因在于，ReplicaSet 不仅仅只管理通过其 podTemplate（`.spec.template`字段）创建的Pod，ReplicaSet 也可能将不是由其创建的 Pod 纳入到自己的管理中。

```yml
apiVersion: v1
kind: Pod
metadata:
  name: pod1
  labels:
    tier: frontend
spec:
  containers:
  - name: hello1
    image: nginx:1.7.9

---

apiVersion: v1
kind: Pod
metadata:
  name: pod2
  labels:
    tier: frontend
spec:
  containers:
  - name: hello2
    image: nginx:1.8.0
```

这些 Pod 没有对应的控制器（或者任何其他对象）作为其 ownerReference，且他们都匹配了 ReplicaSet `frontend` 中的选择器（selector），此时他们将立刻被 ReplicaSet `frontend` 接管。

假设您先创建了上面例子中的 ReplicaSet（其 `replicas` 为 3），此时再执行命令，以创建上面YAML文件中的两个 Pod：

使用 [Horizontal Pod Autoscalers(HPA) (opens new window)](https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/)对 ReplicaSet 执行自动的水平伸缩。下面例子中的 HPA 可以用来对前面例子中的 ReplicaSet 执行自动的水平伸缩：

```yml
apiVersion: autoscaling/v1
kind: HorizontalPodAutoscaler
metadata:
  name: frontend-scaler
spec:
  scaleTargetRef:
    kind: ReplicaSet
    name: frontend
  minReplicas: 3
  maxReplicas: 10
  targetCPUUtilizationPercentage: 50
```

```bash
kubectl autoscale rs frontend --max=10
```

```bash
# 查看 revision（版本）的详细信
kubectl rollout history deployment.v1.apps/nginx-deployment --revision=2
kubectl rollout undo deployment.v1.apps/nginx-deployment
kubectl rollout undo deployment.v1.apps/nginx-deployment --to-revision=2
kubectl autoscale deployment.v1.apps/nginx-deployment --min=10 --max=15 --cpu-percent=80
```

# 金丝雀发布（灰度发布）

```yml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx-deployment
  labels:
    app: nginx
spec:
  replicas: 3
  selector:
    matchLabels:
      app: nginx
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
      - name: nginx
        image: nginx:1.7.9
---
apiVersion: v1
kind: Service
metadata:
  name: nginx-service
  labels:
    app: nginx
spec:
  selector:
    app: nginx
  ports:
  - name: nginx-port
    protocol: TCP
    port: 80
    nodePort: 32600
    targetPort: 80
  type: NodePort
```

```yml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx-deployment-canary
  labels:
    app: nginx
    track: canary
spec:
  replicas: 1
  selector:
    matchLabels:
      app: nginx
      track: canary
  template:
    metadata:
      labels:
        app: nginx
        track: canary
    spec:
      containers:
      - name: nginx
        image: nginx:1.8.0
```

- 因为 Service 的LabelSelector 是 `app: nginx`，由 `nginx-deployment` 和 `nginx-deployment-canary` 创建的 Pod 都带有标签 `app: nginx`，所以，Service 的流量将会在两个 release 之间分配
- 在新旧版本之间，流量分配的比例为两个版本副本数的比例，此处为 1:3

更好一点可以使用Istio 灰度发布

```yml
apiVersion: v1
kind: Service
metadata:
  name: nginx
  labels:
    app: nginx
spec:
  ports:
  - port: 80
    name: web
  clusterIP: None
  selector:
    app: nginx
---
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: web
spec:
  selector:
    matchLabels:
      app: nginx # has to match .spec.template.metadata.labels
  serviceName: "nginx"
  replicas: 3 # by default is 1
  template:
    metadata:
      labels:
        app: nginx # has to match .spec.selector.matchLabels
    spec:
      terminationGracePeriodSeconds: 10
      containers:
      - name: nginx
        image: nginx:1.7.9
        ports:
        - containerPort: 80
          name: web
        volumeMounts:
        - name: www
          mountPath: /usr/share/nginx/html
  volumeClaimTemplates:
  - metadata:
      name: www
    spec:
      accessModes: [ "ReadWriteOnce" ]
      storageClassName: "my-storage-class"
      resources:
        requests:
          storage: 1Gi
```

job 此处只允许使用 `Never` 和 `OnFailure` 两个取值

```bash
pods=$(kubectl get pods --selector=job-name=pi --output=jsonpath='{.items[*].metadata.name}')
echo $pods
```

```yml
apiVersion: batch/v1
kind: Job
metadata:
  name: pi
spec:
  template:
    spec:
      containers:
      - name: pi
        image: perl
        command: ["perl",  "-Mbignum=bpi", "-wle", "print bpi(2000)"]
      restartPolicy: Never
  backoffLimit: 4
```

- Job 中设置了 `.spec.activeDeadlineSeconds`。该字段限定了 Job 对象在集群中的存活时长，一旦达到 `.spec.activeDeadlineSeconds` 指定的时长，该 Job 创建的所有的 Pod 都将被终止，Job 的 Status 将变为 `type:Failed` 、 `reason: DeadlineExceeded`

```yml
apiVersion: batch/v1
kind: Job
metadata:
  name: pi-with-timeout
spec:
  backoffLimit: 5
  activeDeadlineSeconds: 100
  template:
    spec:
      containers:
      - name: pi
        image: perl
        command: ["perl",  "-Mbignum=bpi", "-wle", "print bpi(2000)"]
      restartPolicy: Never
```

系统中已经完成的 Job 通常是不在需要里的，长期在系统中保留这些对象，将给 apiserver 带来很大的压力。如果通过更高级别的控制器（例如 [CronJobs](https://kuboard.cn/learning/k8s-intermediate/workload/wl-cronjob/)）来管理 Job，则 CronJob 可以根据其中定义的基于容量的清理策略（capacity-based cleanup policy）自动清理Job。

```yml
apiVersion: batch/v1
kind: Job
metadata:
  name: pi-with-ttl
spec:
  ttlSecondsAfterFinished: 100
  template:
    spec:
      containers:
      - name: pi
        image: perl
        command: ["perl",  "-Mbignum=bpi", "-wle", "print bpi(2000)"]
      restartPolicy: Never
```

**字段解释 `ttlSecondsAfterFinished`：**

- Job `pi-with-ttl` 的 `ttlSecondsAfterFinished` 值为 100，则，在其结束 `100` 秒之后，将可以被自动删除
- 如果 `ttlSecondsAfterFinished` 被设置为 `0`，则 TTL 控制器在 Job 执行结束后，立刻就可以清理该 Job 及其 Pod
- 如果 `ttlSecondsAfterFinished` 值未设置，则 TTL 控制器不会清理该 Job

#  Network Policies

Kubernetes 中，Network Policy（网络策略）定义了一组 Pod 是否允许相互通信，或者与网络中的其他端点 endpoint 通信。

Network Policy 由网络插件实现，因此，您使用的网络插件必须能够支持 `NetworkPolicy` 才可以使用此特性。如果您仅仅是创建了一个 Network Policy 对象，但是您使用的网络插件并不支持此特性，您创建的 Network Policy 对象是不生效的。

如果一个 NetworkPolicy 的标签选择器选中了某个 Pod，则该 Pod 将变成隔离的（isolated），并将拒绝任何不被 NetworkPolicy 许可的网络连接。（名称空间中其他未被 NetworkPolicy 选中的 Pod 将认可接受来自任何请求方的网络请求。）

Network Police 不会相互冲突，而是相互叠加的。如果多个 NetworkPolicy 选中了同一个 Pod，则该 Pod 可以接受这些 NetworkPolicy 当中任何一个 NetworkPolicy 定义的（入口/出口）规则，是所有NetworkPolicy规则的并集，因此，NetworkPolicy 的顺序并不重要，因为不会影响到最终的结果。

为了使两个容器组之间的网络能够连通，源容器组的出方向网络策略和目标容器组的入方向无网络策略必须同时允许该网络连接。只要其中的任何一方拒绝了该连接，该连接都不能创建成功。

```yml
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: test-network-policy
  namespace: default
spec:
  podSelector:
    matchLabels:
      role: db
  policyTypes:
  - Ingress
  - Egress
  ingress:
  - from:
    - ipBlock:
        cidr: 172.17.0.0/16
        except:
        - 172.17.1.0/24
    - namespaceSelector:
        matchLabels:
          project: myproject
    - podSelector:
        matchLabels:
          role: frontend
    ports:
    - protocol: TCP
      port: 6379
  egress:
  - to:
    - ipBlock:
        cidr: 10.0.0.0/24
    ports:
    - protocol: TCP
      port: 5978
```

## to和from选择器的行为

NetworkPolicy 的 `.spec.ingress.from` 和 `.spec.egress.to` 字段中，可以指定 4 种类型的标签选择器：

- **podSelector** 选择与 `NetworkPolicy` 同名称空间中的 Pod 作为入方向访问控制规则的源或者出方向访问控制规则的目标
- **namespaceSelector** 选择某个名称空间（其中所有的Pod）作为入方向访问控制规则的源或者出方向访问控制规则的目标
- **namespaceSelector** 和 **podSelector** 在一个 `to` / `from` 条目中同时包含 `namespaceSelector` 和 `podSelector` 将选中指定名称空间中的指定 Pod。此时请特别留意 YAML 的写法，如下所示：

```yml
  ...
  ingress:
  - from:
    - namespaceSelector:
        matchLabels:
          user: alice
      podSelector:
        matchLabels:
          role: client
  ...
```

该例子中，podSelector 前面没有 `-` 减号，namespaceSelector 和 podSelector 是同一个 from 元素的两个字段，将选中带 `user=alice` 标签的名称空间中所有带 `role=client` 标签的 Pod。但是，下面的这个 NetworkPolicy 含义是不一样的：

```yml
  ...
  ingress:
  - from:
    - namespaceSelector:
        matchLabels:
          user: alice
    - podSelector:
        matchLabels:
          role: client
  ...
```

后者，podSelector 前面带 `-` 减号，说明 namespaceSelector 和 podSelector 是 from 数组中的两个元素，他们将选中 NetworkPolicy 同名称空间中带 `role=client` 标签的对象，以及带 `user=alice` 标签的名称空间的所有 Pod。

## 默认拒绝所有的入方向流量

在名称空间中创建下面的 NetworkPolicy，该 NetworkPolicy：

- 选中所有的 Pod
- 不允许任何入方向的流量

```yaml
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: default-deny
spec:
  podSelector: {}
  policyTypes:
  - Ingress
 
        已复制到剪贴板！
    
```

此 NetworkPolicy 将确保名称空间中所有的入方向流量都被限制，同时，不改变出方向的流量。

## 默认允许所有的入方向流量默认允许所有的入方向流量

在名称空间中创建下面的 NetworkPolicy，该 NetworkPolicy 允许名称空间中所有 Pod 的所有入方向网络流量

```yaml
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: allow-all
spec:
  podSelector: {}
  ingress:
  - {}
  policyTypes:
  - Ingress
 
        已复制到剪贴板！
    
```

## 默认允许所有出方向流量)默认允许所有出方向流量

在名称空间中创建下面的 NetworkPolicy，该 NetworkPolicy 允许名称空间中所有 Pod 的所有出方向网络流量

```yaml
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: allow-all
spec:
  podSelector: {}
  egress:
  - {}
  policyTypes:
  - Egress
 
        已复制到剪贴板！
    
```

默认拒绝所有入方向和出方向的网络流量)默认拒绝所有入方向和出方向的网络流量

在名称空间中创建下面的 NetworkPolicy，该 NetworkPolicy 禁止名称空间中所有 Pod 的所有入方向流量和所有出方向流量

```yaml
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: default-deny
spec:
  podSelector: {}
  policyTypes:
  - Ingress
  - Egress
 
        已复制到剪贴板！
    
```

## 数据卷内子路径

有时候我们需要在同一个 Pod 的不同容器间共享数据卷。使用 `volumeMounts.subPath` 属性，可以使容器在挂载数据卷时指向数据卷内部的一个子路径，而不是直接指向数据卷的根路径。

下面的例子中，一个 LAMP（Linux Apache Mysql PHP）应用的 Pod 使用了一个共享数据卷，HTML 内容映射到数据卷的 `html` 目录，数据库的内容映射到了 `mysql` 目录：

```yml
apiVersion: v1
kind: Pod
metadata:
  name: my-lamp-site
spec:
    containers:
    - name: mysql
      image: mysql
      env:
      - name: MYSQL_ROOT_PASSWORD
        value: "rootpasswd"
      volumeMounts:
      - mountPath: /var/lib/mysql
        name: site-data
        subPath: mysql
        readOnly: false
    - name: php
      image: php:7.0-apache
      volumeMounts:
      - mountPath: /var/www/html
        name: site-data
        subPath: html
        readOnly: false
    volumes:
    - name: site-data
      persistentVolumeClaim:
        claimName: my-lamp-site-data
```

```yml
apiVersion: v1
kind: Pod
metadata:
  name: pod1
spec:
  containers:
  - name: container1
    env:
    - name: POD_NAME
      valueFrom:
        fieldRef:
          apiVersion: v1
          fieldPath: metadata.name
    image: busybox
    command: [ "sh", "-c", "while [ true ]; do echo 'Hello'; sleep 10; done | tee -a /logs/hello.txt" ]
    volumeMounts:
    - name: workdir1
      mountPath: /logs
      subPathExpr: $(POD_NAME)
      readOnly: false
  restartPolicy: Never
  volumes:
  - name: workdir1
    hostPath:
      path: /var/log/pods
```

## 创建nfs 服务器

```bash
yum install -y rpcbind nfs-utils
cat > /etc/exports <<- eof 
/root/nfs_root/ *(insecure,rw,sync,no_root_squash)
eof
# 创建共享目录，如果要使用自己的目录，请替换本文档中所有的 /root/nfs_root/
mkdir /root/nfs_root
systemctl enable rpcbind
systemctl enable nfs-server
systemctl start rpcbind
systemctl start nfs-server
exportfs -r
exportfs
# 输出结果如下所示
/root/nfs_root /root/nfs_root
```

## 在客户端测试nfs

```bash
yum install -y nfs-utils
# 执行以下命令检查 nfs 服务器端是否有设置共享目录
# showmount -e $(nfs服务器的IP)
showmount -e 172.17.216.82
# 输出结果如下所示
Export list for 172.17.216.82:
/root/nfs_root *
mkdir /root/nfsmount
# mount -t nfs $(nfs服务器的IP):/root/nfs_root /root/nfsmount
mount -t nfs 172.17.216.82:/root/nfs_root /root/nfsmount
# 写入一个测试文件
echo "hello nfs server" > /root/nfsmount/test.txt
在 nfs 服务器上执行以下命令，验证文件写入成功
cat /root/nfs_root/test.txt
```

在 `top` 命令查看CPU消耗，100% 代表 1核；4核为 400%；10% 代表 0.1 核 或者 100m

容忍中未定义 `key` 但是定义了 operator 为 `Exists`，Kubernetes 认为此容忍匹配所有的污点，如下所示：

```bash
tolerations:
- operator: "Exists"
tolerations:
- key: "key"
  operator: "Exists"
```

带有效果 `NoExecute` 的污点还可以指定一个可选字段 `tolerationSeconds`，该字段指定了 Pod 在多长时间后被驱逐，

```yml
tolerations:
- key: "key1"
  operator: "Equal"
  value: "value1"
  effect: "NoExecute"
  tolerationSeconds: 3600
```

```bash
kubectl drain mynode
kubectl drain mynode --as=superman --as-group=system:masters
```

### Role Binding Examples

下面的例子中只显示了 `RoleBinding`、`ClusterRoleBinding` 的 `subjects` 部分。

绑定 user “alice@example.com”：

```yaml
subjects:
- kind: User
  name: "alice@example.com"
  apiGroup: rbac.authorization.k8s.io 
```

绑定 group “frontend-admins”：

```yaml
subjects:
- kind: Group
  name: "frontend-admins"
  apiGroup: rbac.authorization.k8s.io  
```

绑定 kube-system 名称空间中的 default service account：

```yaml
subjects:
- kind: ServiceAccount
  name: default
  namespace: kube-system
```

绑定 qa 名称空间中的所有 service account：

```yaml
subjects:
- kind: Group
  name: system:serviceaccounts:qa
  apiGroup: rbac.authorization.k8s.io
```

绑定任意 service account：

```yaml
subjects:
- kind: Group
  name: system:serviceaccounts
  apiGroup: rbac.authorization.k8s.io
```

绑定所有已认证的用户（kubernetes 1.5+）：

```yaml
subjects:
- kind: Group
  name: system:authenticated
  apiGroup: rbac.authorization.k8s.io
```

绑定所有未认证用户（kubernetes 1.5+）：

```yaml
subjects:
- kind: Group
  name: system:unauthenticated
  apiGroup: rbac.authorization.k8s.io 
```



绑定所有用户（kubernetes 1.5+）：

```yaml
subjects:
- kind: Group
  name: system:authenticated
  apiGroup: rbac.authorization.k8s.io
- kind: Group
  name: system:unauthenticated
  apiGroup: rbac.authorization.k8s.io
```

Limit Range 是一种用来限定名称空间内 Pod（或容器）可以消耗资源数量的策略（Policy）。

LimitRange 对象的 yaml 文件如下所示：

```yml
apiVersion: v1
kind: LimitRange
metadata:
  name: limit-mem-cpu-per-container
spec:
  limits:
  - max:
      cpu: "800m"
      memory: "1Gi"
    min:
      cpu: "100m"
      memory: "99Mi"
    default:
      cpu: "700m"
      memory: "900Mi"
    defaultRequest:
      cpu: "110m"
      memory: "111Mi"
    type: Container
```

# 限定存储资源

本文讨论了如何使用LimitRange_名称空间中限制存储资源的使用。通过 LimitRange 对象，集群管理员可以限定名称空间中每个 PersistentVolumeClaim（存储卷声明）可以使用的最小、最大存储空间。

请参考下面的例子：

```yml
apiVersion: v1
kind: LimitRange
metadata:
  name: storagelimits
spec:
  limits:
  - type: PersistentVolumeClaim
    max:
      storage: 2Gi
    min:
      storage: 1Gi
```

```yml
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: pvc-limit-lower
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 500Mi
```

```yml
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: pvc-limit-greater
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 5Gi
```

执行命令创建该 PVC（存储卷声明）创建失败

# 限定 Limit/Request 比例

> 参考文档：[Limit Ranges(opens new window)](https://kubernetes.io/docs/concepts/policy/limit-range/)

本文讨论了如何使用 LimitRange 在名称空间中限制 Limits/Requests 的比例。如果指定了 LimitRange 对象的 `spec.limits.maxLimitRequestRatio` 字段，名称空间中的 Pod/容器的 request 和 limit 都不能为 0，且 limit 除以 request 的结果必须小于或等于 LimitRange 的 `spec.limits.maxLimitRequestRatio`

下面的例子中 `LimitRange` 限定了名称空间中任何 Pod 的最大内存限定（limit）不能超过最小内存请求（request）的两倍：

```yaml
apiVersion: v1
kind: LimitRange
metadata:
  name: limit-memory-ratio-pod
spec:
  limits:
  - maxLimitRequestRatio:
      memory: 2
    type: Pod
```

- 执行命令以创建该 LimitRange：

  ```sh
  kubectl create -f https://kuboard.cn/statics/learning/policy/lr-ratio-limit-range.yaml -n limitrange-demo  
  ```

  执行命令以查看创建结果：

  ```sh
  kubectl describe limitrange/limit-memory-ratio-pod -n limitrange-demo
  ```

  输出结果如下所示：

  ```text
  Name:       limit-memory-ratio-pod
  Namespace:  limitrange-demo
  Type        Resource  Min  Max  Default Request  Default Limit  Max Limit/Request Ratio
  ----        --------  ---  ---  ---------------  -------------  -----------------------
  Pod         memory    -    -    -                -              2 
  ```

- 此时，如果我们创建一个 Pod 包含如下属性 `requests.memory=100Mi` 和 `limits.memory=300Mi`：

  ```yaml
  apiVersion: v1
  kind: Pod
  metadata:
    name: busybox3
  spec:
    containers:
    - name: busybox-cnt01
      image: busybox
      resources:
        limits:
          memory: "300Mi"
        requests:
          memory: "100Mi"  
  ```

  执行命令以创建该 Pod：

  ```sh
  kubectl apply -f https://kuboard.cn/statics/learning/policy/lr-ratio-pod.yaml -n limitrange-demo 
  ```

  由于该 Pod 的内存限制请求比例为 `3`，超过了 LimitRange 中定义的 `2`，该 Pod 将不能创建成功：

  ```text
  Error from server (Forbidden): error when creating "lr-ratio-pod.yaml": pods "busybox3" is forbidden: memory max limit to request ratio per Pod is 2, but provided ratio is 3.000000.
  ```

# 限定 Limit/Request 比例

> 参考文档：[Limit Ranges(opens new window)](https://kubernetes.io/docs/concepts/policy/limit-range/)

本文讨论了如何使用 LimitRange 在名称空间中限制 Limits/Requests 的比例。如果指定了 LimitRange 对象的 `spec.limits.maxLimitRequestRatio` 字段，名称空间中的 Pod/容器的 request 和 limit 都不能为 0，且 limit 除以 request 的结果必须小于或等于 LimitRange 的 `spec.limits.maxLimitRequestRatio`

下面的例子中 `LimitRange` 限定了名称空间中任何 Pod 的最大内存限定（limit）不能超过最小内存请求（request）的两倍：

```yaml
apiVersion: v1
kind: LimitRange
metadata:
  name: limit-memory-ratio-pod
spec:
  limits:
  - maxLimitRequestRatio:
      memory: 2
    type: Pod
```

- 执行命令以创建该 LimitRange：

  ```sh
  kubectl create -f https://kuboard.cn/statics/learning/policy/lr-ratio-limit-range.yaml -n limitrange-demo
  ```

  1

  执行命令以查看创建结果：

  ```sh
  kubectl describe limitrange/limit-memory-ratio-pod -n limitrange-demo
  ```

  ```text
  Name:       limit-memory-ratio-pod
  Namespace:  limitrange-demo
  Type        Resource  Min  Max  Default Request  Default Limit  Max Limit/Request Ratio
  ----        --------  ---  ---  ---------------  -------------  -----------------------
  Pod         memory    -    -    -                -              2
  ```

- 此时，如果我们创建一个 Pod 包含如下属性 `requests.memory=100Mi` 和 `limits.memory=300Mi`：

  ```yaml
  apiVersion: v1
  kind: Pod
  metadata:
    name: busybox3
  spec:
    containers:
    - name: busybox-cnt01
      image: busybox
      resources:
        limits:
          memory: "300Mi"
        requests:
          memory: "100Mi"
  ```

  执行命令以创建该 Pod：

  ```sh
  kubectl apply -f https://kuboard.cn/statics/learning/policy/lr-ratio-pod.yaml -n limitrange-demo 
  ```

  由于该 Pod 的内存限制请求比例为 `3`，超过了 LimitRange 中定义的 `2`，该 Pod 将不能创建成功：

  ```text
  Error from server (Forbidden): error when creating "lr-ratio-pod.yaml": pods "busybox3" is forbidden: memory max limit to request ratio per Pod is 2, but provided ratio is 3.000000.
  ```

`备份脚本`（k8s-master1 机器上备份）：

```bash
#!/usr/bin/env bash

date;

CACERT="/opt/kubernetes/ssl/ca.pem"
CERT="/opt/kubernetes/ssl/server.pem"
EKY="/opt/kubernetes/ssl/server-key.pem"
ENDPOINTS="192.168.1.36:2379"

ETCDCTL_API=3 etcdctl \
--cacert="${CACERT}" --cert="${CERT}" --key="${EKY}" \
--endpoints=${ENDPOINTS} \
snapshot save /data/etcd_backup_dir/etcd-snapshot-`date +%Y%m%d`.db

# 备份保留30天
find /data/etcd_backup_dir/ -name *.db -mtime +30 -exec rm -f {} \;
```

### Get the external IP of all nodes

```bash
kubectl get nodes -o jsonpath='{.items[*].status.addresses[?(@.type=="ExternalIP")].address}'
```

```ba
kubectl get services --sort.by=.metadata.name
kubectl annotate deploy nginx mycompany.com/someannotation="chad"
kubectl get po --field-selector status.phase=Running
systemctl list-unit-files --type service --all | grep kube
kubectl apply -f dir1 -R
```

::: center

::: tip
这是一条提示
:::



```

```

```

```